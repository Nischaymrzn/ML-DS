# 📘 Linear Regression Optimization using Gradient Descent

Today, I deepened my understanding of how **Linear Regression** is optimized using **Gradient Descent**. 

---

## 🔄 Gradient Descent: Core Idea
- Gradient Descent is an **optimization algorithm** used to minimize the cost function in machine learning.
- It works by **iteratively adjusting model parameters** (like weights) in the direction that reduces the cost.
- The goal is to find parameters that result in the **best fit line** for the training data.

---

## 🧮 Cost Function (Mean Squared Error)
- Measures the difference between predicted and actual values.
- Gradient Descent attempts to minimize this function.

---

## ⚙️ Learning Rate & Convergence
- **Learning Rate (α):** Controls how big the steps are during optimization.
  - Too small → Slow convergence.
  - Too large → May overshoot or diverge.
- It's essential to choose a **balanced learning rate** to ensure efficient and stable training.

---

## 🔍 Key Takeaways
- Gradient Descent is central to training regression models efficiently.
- Proper tuning of the learning rate significantly affects the model's performance and training speed.
- Once parameters are optimized, they can be used to make predictions on new, unseen data.

---

## 📌 Summary
- Understood how gradient descent adjusts parameters to minimize error.
- Learned the role of the learning rate in convergence.
- Practically applied the concept by optimizing a linear regression model.
- Used the optimized parameters for predictions.

---

