# 📘 Topic: Automatic Differentiation, Model Evaluation, and Introduction to Integrals

---

## ✅ Today's Learning Objectives Completed

- Practiced computing gradients using tensors for model learning  
- Explored how confusion matrices evaluate classification performance  
- Started learning about integrals and their connection to accumulation and area  

---

## 📝 Detailed Notes

### 🔹 Automatic Differentiation with Tensors

- Worked with tensors to **compute gradients automatically**  
- Observed how **backpropagation** relies on gradient flow through tensor operations  
- Reinforced understanding of how models "learn" by updating parameters using gradients  

---

### 🔹 Confusion Matrix Overview

- A confusion matrix summarizes **classification performance**:  
  \[
  \begin{bmatrix}
  \text{TP} & \text{FP} \\
  \text{FN} & \text{TN}
  \end{bmatrix}
  \]  
- Key metrics:
  - **Accuracy** = (TP + TN) / Total  
  - **Precision** = TP / (TP + FP)  
  - **Recall** = TP / (TP + FN)  
  - **F1-Score** = Harmonic mean of Precision and Recall  
- Useful for analyzing imbalanced datasets and model behavior  

---

### 🔹 Intro to Integrals

- Integrals represent **accumulated quantities**—e.g., area under a curve  
- The **definite integral** gives the total accumulation over an interval  
- Initial insight into how integrals relate to **probability**, **density functions**, and **loss analysis** in ML  

---

## 🔑 Key Takeaways

- Tensor-based auto-differentiation powers model training in modern ML frameworks  
- Confusion matrices provide detailed insights into classifier strengths and weaknesses  
- Integration begins the journey into **area-based reasoning**, important in probability and continuous models  
