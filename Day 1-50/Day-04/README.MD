### Day 4  
**Topic:** Orthogonal Matrices, Gram-Schmidt Process, Eigen Concepts  
**Date:** April 17, 2025  

---

**Orthogonal Matrices:**

Definition: Learned that a matrix is orthogonal if its transpose is equal to its inverse (\( Q^T = Q^{-1} \)).  
Properties: Discovered that the rows and columns of orthogonal matrices are orthonormal—mutually perpendicular and of unit length.  
Applications: Useful in preserving lengths and angles during transformations. Commonly used in graphics, optimization, and ML for stable and efficient computations.

---

**Gram-Schmidt Process:**

Goal: Learned how to convert a set of linearly independent vectors into an orthonormal set.  
Process: Applied projections to remove components in the direction of earlier vectors, then normalized the result.  
Significance: Forms the basis for QR decomposition and helps simplify linear transformations by building orthonormal bases.

---

**Eigenvalues and Eigenvectors:**

Eigenvectors: Understood that eigenvectors remain in the same direction after a transformation—they only scale.  
Eigenvalues: Learned that eigenvalues are the factors by which eigenvectors are stretched or compressed.  
Applications: Found in PCA, system stability analysis, and many ML algorithms where data transformation or compression is involved.

---

**Eigenbasis:**

Definition: Discovered that an eigenbasis is a set of eigenvectors that spans the vector space.  
Diagonalization: Learned how a matrix with a full set of eigenvectors can be diagonalized as \( A = PDP^{-1} \), simplifying computations.  
Importance: Essential in transforming complex systems into simpler, decoupled forms. Very useful in ML, physics, and differential equations.
